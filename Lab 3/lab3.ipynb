{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 A. \n",
    "Data exploration\n",
    "Explore the three datasets. Use your judgment as a human being to describe\n",
    "what makes you able to tell spam apart from ham. Also explore the differences between the two classes of ham (easy and hard ham). What makes them different? Write approximately one paragraph.\n",
    "\n",
    "1. ##### Difference between Ham and Spam\n",
    "    Spam contains a lot of clickbaity titles with links that looks more than suspicious. Very sensational language and the senders adress looks very auto-generated for example: sabrina@mx3.1premio.com.\n",
    "    Ham on the other hand is more legit and the emails looks like they are sent from either real people or real companies with a newsletter and you get the choice to unsubscribe or not.\n",
    "\n",
    "2. ##### Difference between Hard and Easy Ham\n",
    "    Easy ham seems to be mostly conversations between real life people, not alot of code, mostly natural speaking language\n",
    "    Hard Ham contains more code, and contains mostly of commercials which makes it hard to judge if it's actually wanted by the reciever or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 B.\n",
    "Perform an appropriate train-test split on the each of the datasets. We will use\n",
    "the training sets to train a classifier, and evaluate its performance against the\n",
    "test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'c:\\\\Users\\\\norlandw\\\\Desktop\\\\DIT-407-Data-Science-and-AI\\\\Lab 3\\\\20021010_easy_ham\\\\easy_ham\\\\0001.f0cf04027e74802f09f723cb8916b48e'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m     f\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_name \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(hard_ham_dir_path):\n\u001b[1;32m---> 25\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43measy_ham_dir_path\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mfile_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlatin-1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m     text \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m     27\u001b[0m     hard_ham_texts\u001b[38;5;241m.\u001b[39mappend(text)\n",
      "File \u001b[1;32mc:\\Users\\norlandw\\Desktop\\DIT-407-Data-Science-and-AI\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'c:\\\\Users\\\\norlandw\\\\Desktop\\\\DIT-407-Data-Science-and-AI\\\\Lab 3\\\\20021010_easy_ham\\\\easy_ham\\\\0001.f0cf04027e74802f09f723cb8916b48e'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Creating lists and dicts to parse the text files\n",
    "easy_ham_texts: list[str] = []\n",
    "hard_ham_texts: list[str] = []\n",
    "spam_texts: list[str] = []\n",
    "easy_ham_labels: list[str] = []\n",
    "hard_ham_labels: list[str] = []\n",
    "spam_labels: list[str] = []\n",
    "easy_ham_dir_path = os.getcwd() + \"\\\\20021010_easy_ham\\\\easy_ham\\\\\"\n",
    "hard_ham_dir_path = os.getcwd() + \"\\\\20021010_hard_ham\\\\hard_ham\\\\\"\n",
    "spam_dir_path = os.getcwd() + \"\\\\20021010_spam\\\\spam\\\\\"\n",
    "\n",
    "# Parsing the text files into lists\n",
    "for file_name in os.listdir(easy_ham_dir_path):\n",
    "    f = open(easy_ham_dir_path+file_name, \"r\", encoding=\"latin-1\")\n",
    "    text = f.read()\n",
    "    easy_ham_texts.append(text)\n",
    "    easy_ham_labels.append(\"easy_ham\")\n",
    "    f.close()\n",
    "\n",
    "for file_name in os.listdir(hard_ham_dir_path):\n",
    "    f = open(easy_ham_dir_path+file_name, \"r\", encoding=\"latin-1\")\n",
    "    text = f.read()\n",
    "    hard_ham_texts.append(text)\n",
    "    hard_ham_labels.append(\"hard_ham\")\n",
    "    f.close()\n",
    "\n",
    "for file_name in os.listdir(spam_dir_path):\n",
    "    f = open(easy_ham_dir_path+file_name, \"r\", encoding=\"latin-1\")\n",
    "    text = f.read()\n",
    "    spam_texts.append(text)\n",
    "    spam_labels.append(\"spam\")\n",
    "    f.close()\n",
    "\n",
    "# type hinting for the lists\n",
    "easy_ham_x_test: list[str]\n",
    "easy_ham_y_test: list[str]\n",
    "easy_ham_x_train: list[str]\n",
    "easy_ham_y_train: list[str]\n",
    "hard_ham_x_test: list[str]\n",
    "hard_ham_y_test: list[str]\n",
    "hard_ham_x_train: list[str]\n",
    "hard_ham_y_train: list[str]\n",
    "spam_x_test: list[str]\n",
    "spam_y_test: list[str]\n",
    "spam_x_train: list[str]\n",
    "spam_y_train: list[str]\n",
    "\n",
    "# Make a simple train-test split using 25% of the data for testing as this was recommended in last thursday's lecture,\n",
    "# no shuffling is needed for naive bayes\n",
    "easy_ham_x_train, easy_ham_x_test, easy_ham_y_train, easy_ham_y_test = train_test_split(easy_ham_texts, easy_ham_labels, test_size=0.25)\n",
    "\n",
    "hard_ham_x_train, hard_ham_x_test, hard_ham_y_train, hard_ham_y_test = train_test_split(hard_ham_texts, hard_ham_labels, test_size=0.25)\n",
    "\n",
    "spam_x_train, spam_x_test, spam_y_train, spam_y_test = train_test_split(spam_texts, spam_labels, test_size=0.25)\n",
    "\n",
    "print(spam_x_test)\n",
    "print(spam_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2.\n",
    "A bag of words maps text documents into vectors of word counts. That is, each\n",
    "word is assigned a token number, and we count the number of occurrences of\n",
    "each unique word in each document. The vector then consists of these counts.\n",
    "Use the CountVectorizer class in Scikit-Learn to convert the emails to\n",
    "vectors (or rather the set of emails into a matrix). Acquaint yourself with the\n",
    "documentation of the class because it is nontrivial to use (specifically understand\n",
    "what the methods fit, transform, and fit transform do and how to use them\n",
    "in this setting). In your report, explain succinctly what you did. You donâ€™t have\n",
    "to separate the headers from the body of the email; processing the entire email\n",
    "is ok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse._csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(encoding='latin-1')\n",
    "\n",
    "# Combine spam and ham into one list for fitting the vectorizer to produce matrixes\n",
    "vectorizer_fitting_data = []\n",
    "vectorizer_fitting_data.extend(easy_ham_x_train)\n",
    "vectorizer_fitting_data.extend(spam_x_train)\n",
    "vectorizer_fitting_data.extend(hard_ham_x_train)\n",
    "\n",
    "# Fit the vectorizer to the training data.\n",
    "vectorizer = vectorizer.fit(vectorizer_fitting_data)\n",
    "\n",
    "# Transform the training data into a matrix.\n",
    "easy_ham_x_train_matrix: csr_matrix = vectorizer.transform(easy_ham_x_train)\n",
    "spam_x_train_matrix: csr_matrix = vectorizer.transform(spam_x_train)\n",
    "hard_ham_x_train_matrix: csr_matrix = vectorizer.transform(hard_ham_x_train)\n",
    "\n",
    "# Transform the test data into a matrix.\n",
    "easy_ham_x_test_matrix: csr_matrix = vectorizer.transform(easy_ham_x_test)\n",
    "spam_x_test_matrix: csr_matrix = vectorizer.transform(spam_x_test)\n",
    "hard_ham_x_test_matrix: csr_matrix = vectorizer.transform(hard_ham_x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3.\n",
    "We are going to try out two different Na ÌˆÄ±ve Bayesian Classifiers: <br>\n",
    "<a>1https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.</a> <br>\n",
    "text.CountVectorizer.html\n",
    "\n",
    "* The Multinomial Naive Bayesian Classifier, and\n",
    "* The Bernoulli Naive Bayesian Classifier.\n",
    "\n",
    "For this problem, use the set easy ham as your ham. Use the training sets\n",
    "of ham and spam to train an instance of both classifiers. Then, evaluate the\n",
    "classifier against the test set.\n",
    "In your report, report the accuracy, precision, and recall of your classifiers,\n",
    "that is, six numbers. Also provide the confusion matrix for both classifiers (2Ã—2\n",
    "matrix where positive/negative predictions are the columns, and actual posi-\n",
    "tive/negative values as rows). The count of true/false positives and true/false\n",
    "negatives should be easily readable in the matrix, together with the marginal\n",
    "sums."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "import numpy as np\n",
    "\n",
    "# Create the training data.\n",
    "training_data = np.concatenate((easy_ham_x_train_matrix.toarray(), spam_x_train_matrix.toarray()))\n",
    "training_labels = np.concatenate((easy_ham_y_train, spam_y_train))\n",
    "\n",
    "# Create the classifiers and fit it to the training data.\n",
    "multinomial_nb_clf = MultinomialNB()\n",
    "multinomial_nb_clf.fit(training_data, training_labels)\n",
    "\n",
    "bernoulli_nb_clf = BernoulliNB()\n",
    "bernoulli_nb_clf.fit(training_data, training_labels)\n",
    "\n",
    "# Create the test data.\n",
    "test_data = np.concatenate((easy_ham_x_test_matrix.toarray(), spam_x_test_matrix.toarray()))\n",
    "test_labels = np.concatenate((easy_ham_y_test, spam_y_test))\n",
    "\n",
    "# Evaluate accuracy on the test data.\n",
    "# accuracy answers the question: how often the model is right?\n",
    "bernoulli_accuracy_score = bernoulli_nb_clf.score(test_data, test_labels)\n",
    "multinomial_accuracy_score = multinomial_nb_clf.score(test_data, test_labels)\n",
    "\n",
    "print(f\"MultinomialNB accuracy: {multinomial_accuracy_score}\")\n",
    "print(f\"BernoulliNB accuracy: {bernoulli_accuracy_score}\")\n",
    "\n",
    "# Evaluate the precision, figured average = \"binary\" is fine since this is a two class classification problem.\n",
    "# precision answers the question: how often the positive predictions are correct?\n",
    "easy_ham_precision_multinomial = precision_score(easy_ham_y_test, multinomial_nb_clf.predict(easy_ham_x_test_matrix), pos_label=\"easy_ham\")\n",
    "spam_precision_multinomial = precision_score(spam_y_test, multinomial_nb_clf.predict(spam_x_test_matrix), pos_label=\"spam\")\n",
    "easy_ham_precision_bernoulli = precision_score(easy_ham_y_test, bernoulli_nb_clf.predict(easy_ham_x_test_matrix), pos_label=\"easy_ham\")\n",
    "spam_precision_bernoulli = precision_score(spam_y_test, bernoulli_nb_clf.predict(spam_x_test_matrix), pos_label=\"spam\")\n",
    "\n",
    "print(f\"MultinomialNB easy_ham precision: {easy_ham_precision_multinomial}\")\n",
    "print(f\"MultinomialNB spam precision: {spam_precision_multinomial}\")\n",
    "print(f\"BernoulliNB easy_ham precision: {easy_ham_precision_bernoulli}\")\n",
    "print(f\"BernoulliNB spam precision: {spam_precision_bernoulli}\")\n",
    "\n",
    "# Evaluate the recall \n",
    "# recall answers the question: can an ML model find all instances of the positive class?\n",
    "easy_ham_recall_multinomial = recall_score(easy_ham_y_test, multinomial_nb_clf.predict(easy_ham_x_test_matrix), pos_label=\"easy_ham\", zero_division=0)\n",
    "spam_recall_multinomial = recall_score(spam_y_test, multinomial_nb_clf.predict(spam_x_test_matrix), pos_label=\"spam\", zero_division=0)\n",
    "easy_ham_recall_bernoulli = recall_score(easy_ham_y_test, bernoulli_nb_clf.predict(easy_ham_x_test_matrix), pos_label=\"easy_ham\", zero_division=0)\n",
    "spam_recall_bernoulli = recall_score(spam_y_test, bernoulli_nb_clf.predict(spam_x_test_matrix), pos_label=\"spam\", zero_division=0)\n",
    "\n",
    "print(f\"MultinomialNB easy_ham recall: {easy_ham_recall_multinomial}\")\n",
    "print(f\"MultinomialNB spam recall: {spam_recall_multinomial}\")\n",
    "print(f\"BernoulliNB easy_ham recall: {easy_ham_recall_bernoulli}\")\n",
    "print(f\"BernoulliNB spam recall: {spam_recall_bernoulli}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
